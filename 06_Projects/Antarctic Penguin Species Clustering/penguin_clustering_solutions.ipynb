{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eaa5eff",
   "metadata": {},
   "source": [
    "# Antarctic Penguin Species Clustering - Practice Solutions\n",
    "\n",
    "## Project Overview\n",
    "- **Date**: May 13, 2025\n",
    "- **Objective**: Apply unsupervised learning techniques to cluster penguin data and compare algorithm-derived clusters with actual taxonomic classifications.\n",
    "- **Data Source**: Palmer Penguins Dataset\n",
    "\n",
    "This notebook provides solutions to the mini practice tasks outlined in the project README. It focuses on advanced clustering techniques and alternative dimensionality reduction methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2a1f28",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28946ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "sns.set(font_scale=1.2)\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71205ec",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f839ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "penguins = pd.read_csv('penguins.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Dataset shape: {penguins.shape}\")\n",
    "penguins.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e0aa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "# Create a clean copy of the dataset\n",
    "penguins_clean = penguins.copy()\n",
    "\n",
    "# Handle missing values\n",
    "penguins_clean = penguins_clean.dropna()\n",
    "print(f\"Cleaned dataset shape: {penguins_clean.shape}\")\n",
    "\n",
    "# Select features for clustering\n",
    "feature_cols = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "X = penguins_clean[feature_cols]\n",
    "\n",
    "# Store the actual species labels for later comparison\n",
    "species_labels = penguins_clean['species']\n",
    "\n",
    "# Map species to numeric values for comparison metrics\n",
    "species_map = {'Adelie': 0, 'Chinstrap': 1, 'Gentoo': 2}\n",
    "y_true = penguins_clean['species'].map(species_map)\n",
    "\n",
    "# Standardize features (important for clustering)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA for dimensionality reduction (to use in visualizations)\n",
    "pca = PCA(n_components=2)\n",
    "pca_result = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"Explained variance with 2 PCA components: {pca.explained_variance_ratio_.sum()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641dc3ab",
   "metadata": {},
   "source": [
    "## Mini Practice Task 1: t-SNE for Dimensionality Reduction\n",
    "\n",
    "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a nonlinear dimensionality reduction technique that is particularly well-suited for visualizing high-dimensional data in a low-dimensional space. Unlike PCA, which preserves global structure, t-SNE focuses on preserving local structure, making it excellent for visualizing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd31c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for dimensionality reduction\n",
    "# Since t-SNE can be sensitive to hyperparameters, we'll try a few perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "fig, axes = plt.subplots(1, len(perplexities), figsize=(18, 6))\n",
    "\n",
    "for i, perplexity in enumerate(perplexities):\n",
    "    # Apply t-SNE with the current perplexity\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, n_iter=1000)\n",
    "    tsne_result = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    # Plot the results\n",
    "    scatter = axes[i].scatter(tsne_result[:, 0], tsne_result[:, 1], \n",
    "                             c=y_true, cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "    axes[i].set_title(f't-SNE with Perplexity = {perplexity}')\n",
    "    axes[i].set_xlabel('t-SNE Component 1')\n",
    "    axes[i].set_ylabel('t-SNE Component 2')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Create a legend with species names\n",
    "species_names = list(species_map.keys())\n",
    "handles, _ = scatter.legend_elements()\n",
    "fig.legend(handles, species_names, title=\"Species\", loc='upper center', \n",
    "           bbox_to_anchor=(0.5, 0.05), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb594c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare t-SNE with PCA side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Plot PCA results\n",
    "scatter1 = axes[0].scatter(pca_result[:, 0], pca_result[:, 1], c=y_true, \n",
    "                          cmap='viridis', s=100, alpha=0.8, edgecolor='k')\n",
    "axes[0].set_title('PCA Projection')\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Use the best t-SNE from above (perplexity = 30 is a good default)\n",
    "tsne = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "tsne_result = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# Plot t-SNE results\n",
    "scatter2 = axes[1].scatter(tsne_result[:, 0], tsne_result[:, 1], c=y_true, \n",
    "                          cmap='viridis', s=100, alpha=0.8, edgecolor='k')\n",
    "axes[1].set_title('t-SNE Projection')\n",
    "axes[1].set_xlabel('t-SNE Component 1')\n",
    "axes[1].set_ylabel('t-SNE Component 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add shared legend\n",
    "handles, _ = scatter1.legend_elements()\n",
    "fig.legend(handles, species_names, title=\"Species\", loc='upper center', \n",
    "           bbox_to_anchor=(0.5, 0.05), ncol=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store t-SNE results for later use\n",
    "best_tsne_result = tsne_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec41b6",
   "metadata": {},
   "source": [
    "### Analysis of PCA vs t-SNE\n",
    "\n",
    "**PCA:**\n",
    "- Linear dimensionality reduction that preserves global structure\n",
    "- Maintains maximum variance in the data\n",
    "- Typically faster and more deterministic than t-SNE\n",
    "- Works well when the data has a linear structure\n",
    "\n",
    "**t-SNE:**\n",
    "- Nonlinear dimensionality reduction that preserves local structure\n",
    "- Better at revealing clusters and patterns in complex, high-dimensional data\n",
    "- More computationally intensive than PCA\n",
    "- Results can vary based on perplexity and random initialization\n",
    "- Often provides better separation between clusters\n",
    "\n",
    "In this case, t-SNE appears to provide a clearer separation of the penguin species compared to PCA, especially for distinguishing between Adelie and Chinstrap penguins. This suggests that there are nonlinear relationships in the penguin measurement data that t-SNE captures better than PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53212e81",
   "metadata": {},
   "source": [
    "## Mini Practice Task 2: Using Different Distance Metrics for Hierarchical Clustering\n",
    "\n",
    "This task explores how different distance metrics affect hierarchical clustering results. We'll compare three common distance metrics:\n",
    "\n",
    "1. **Euclidean distance**: Straight-line distance between two points (L2 norm)\n",
    "2. **Manhattan distance**: Sum of absolute differences (L1 norm)\n",
    "3. **Cosine distance**: Measures angle between vectors regardless of magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda7beec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distance metrics to test\n",
    "dist_metrics = ['euclidean', 'manhattan', 'cosine']\n",
    "\n",
    "# Define linkage methods\n",
    "link_method = 'ward'  # Ward's method minimizes variance within clusters\n",
    "\n",
    "# Create a figure for dendrograms\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Store ARI scores for comparison\n",
    "ari_scores = []\n",
    "\n",
    "for i, metric in enumerate(dist_metrics):\n",
    "    # For Ward linkage, only Euclidean distance is valid, so use complete linkage for others\n",
    "    method = 'ward' if metric == 'euclidean' else 'complete'\n",
    "    \n",
    "    # Calculate distance matrix with current metric\n",
    "    if metric == 'euclidean':\n",
    "        Z = linkage(X_scaled, method=method, metric=metric)\n",
    "    else:\n",
    "        # For non-Euclidean metrics, compute the distance matrix first\n",
    "        dist_matrix = pdist(X_scaled, metric=metric)\n",
    "        Z = linkage(dist_matrix, method=method)\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    plt.subplot(len(dist_metrics), 2, 2*i+1)\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram ({metric} distance)', fontsize=14)\n",
    "    plt.xlabel('Sample Index', fontsize=12)\n",
    "    plt.ylabel('Distance', fontsize=12)\n",
    "    dendrogram(\n",
    "        Z,\n",
    "        leaf_rotation=90.,\n",
    "        leaf_font_size=8.,\n",
    "    )\n",
    "    plt.axhline(y=5, color='r', linestyle='--')  # Cut line\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Cut the dendrogram to get cluster labels (3 clusters to match species count)\n",
    "    hc_labels = fcluster(Z, t=3, criterion='maxclust')  \n",
    "    \n",
    "    # Calculate Adjusted Rand Index (ARI) to compare with true species\n",
    "    ari = adjusted_rand_score(y_true, hc_labels)\n",
    "    ari_scores.append(ari)\n",
    "    \n",
    "    # Plot the clusters in PCA space\n",
    "    plt.subplot(len(dist_metrics), 2, 2*i+2)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=hc_labels, \n",
    "                cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "    plt.title(f'Hierarchical Clusters with {metric} distance (ARI: {ari:.3f})', fontsize=14)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print out comparison of metrics by ARI score\n",
    "metrics_comparison = pd.DataFrame({\n",
    "    'Distance Metric': dist_metrics,\n",
    "    'Adjusted Rand Index': ari_scores\n",
    "}).sort_values(by='Adjusted Rand Index', ascending=False)\n",
    "\n",
    "print(\"\\nDistance Metrics Performance Comparison (higher ARI is better):\")\n",
    "print(metrics_comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1409fbe",
   "metadata": {},
   "source": [
    "### Analysis of Different Distance Metrics\n",
    "\n",
    "The three distance metrics we tested (Euclidean, Manhattan, and Cosine) each have different properties that affect clustering results:\n",
    "\n",
    "- **Euclidean distance** works well when clusters are spherical and features are on similar scales (which we've ensured through standardization). It's sensitive to both direction and magnitude differences.\n",
    "\n",
    "- **Manhattan distance** measures the sum of absolute differences along each dimension, making it less sensitive to outliers than Euclidean distance. It works better when clusters are more aligned with the coordinate axes.\n",
    "\n",
    "- **Cosine distance** focuses only on the angle between samples as vectors, ignoring magnitude. This can be useful when we care about the pattern or direction of features rather than their absolute values.\n",
    "\n",
    "Based on the Adjusted Rand Index (ARI) scores, which measure the similarity between the clustering results and the true species labels, we can determine which distance metric best recovered the natural penguin species groupings. The metric with the highest ARI score provides clusters that most closely match the actual taxonomic classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a99a84",
   "metadata": {},
   "source": [
    "## Mini Practice Task 3: Gaussian Mixture Models (GMM)\n",
    "\n",
    "Gaussian Mixture Models (GMM) provide a probabilistic approach to clustering. Unlike K-means, which assigns each data point to exactly one cluster, GMM calculates probabilities of cluster membership, allowing for soft clustering. GMM also captures clusters with different sizes and elliptical shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a8e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal number of components for GMM using BIC (Bayesian Information Criterion)\n",
    "n_components_range = range(1, 10)\n",
    "bic_scores = []\n",
    "aic_scores = []\n",
    "\n",
    "for n_components in n_components_range:\n",
    "    # Train GMM\n",
    "    gmm = GaussianMixture(n_components=n_components, \n",
    "                          covariance_type='full', \n",
    "                          random_state=42,\n",
    "                          max_iter=200,\n",
    "                          n_init=10)\n",
    "    gmm.fit(X_scaled)\n",
    "    \n",
    "    # Store scores\n",
    "    bic_scores.append(gmm.bic(X_scaled))\n",
    "    aic_scores.append(gmm.aic(X_scaled))\n",
    "\n",
    "# Plot BIC and AIC scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(n_components_range, bic_scores, 'o-', label='BIC')\n",
    "plt.plot(n_components_range, aic_scores, 'o-', label='AIC')\n",
    "plt.xlabel('Number of Components', fontsize=14)\n",
    "plt.ylabel('Information Criterion Score', fontsize=14)\n",
    "plt.title('BIC and AIC Scores for Different Numbers of GMM Components', fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(n_components_range)\n",
    "plt.show()\n",
    "\n",
    "# Find the optimal number of components based on BIC\n",
    "optimal_n_components = n_components_range[np.argmin(bic_scores)]\n",
    "print(f\"Optimal number of components based on BIC: {optimal_n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a77c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply GMM with optimal number of components\n",
    "# However, we'll also try with 3 components to match the number of actual species\n",
    "n_components_to_try = [optimal_n_components, 3]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "ari_gmm_scores = []\n",
    "\n",
    "for i, n in enumerate(n_components_to_try):\n",
    "    # Train GMM\n",
    "    gmm = GaussianMixture(n_components=n, \n",
    "                          covariance_type='full', \n",
    "                          random_state=42,\n",
    "                          max_iter=200,\n",
    "                          n_init=10)\n",
    "    gmm_labels = gmm.fit_predict(X_scaled)\n",
    "    \n",
    "    # Get probabilities for each point\n",
    "    proba = gmm.predict_proba(X_scaled)\n",
    "    \n",
    "    # Calculate ARI\n",
    "    ari = adjusted_rand_score(y_true, gmm_labels)\n",
    "    ari_gmm_scores.append(ari)\n",
    "    \n",
    "    # Create subplots: one for the clusters, one for the probability distribution\n",
    "    plt.subplot(2, 2, i*2+1)\n",
    "    plt.scatter(pca_result[:, 0], pca_result[:, 1], c=gmm_labels, \n",
    "                cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "    plt.title(f'GMM with {n} Components (ARI: {ari:.3f})', fontsize=14)\n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.2f}%)', fontsize=12)\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.2f}%)', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot cluster membership probabilities for the first 20 samples\n",
    "    plt.subplot(2, 2, i*2+2)\n",
    "    if n <= 10:  # Only plot if not too many components\n",
    "        sample_proba_df = pd.DataFrame(proba[:20], \n",
    "                                     columns=[f'Cluster {i}' for i in range(n)])\n",
    "        sns.heatmap(sample_proba_df, cmap='viridis', annot=True, fmt='.2f', cbar=True)\n",
    "        plt.title(f'Cluster Membership Probabilities (First 20 Samples)', fontsize=14)\n",
    "        plt.xlabel('Cluster', fontsize=12)\n",
    "        plt.ylabel('Sample Index', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Store the GMM labels with 3 components for later comparison\n",
    "gmm_final = GaussianMixture(n_components=3, covariance_type='full', random_state=42, n_init=10)\n",
    "gmm_labels = gmm_final.fit_predict(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26bdc3b",
   "metadata": {},
   "source": [
    "## Comparing All Clustering Methods\n",
    "\n",
    "Now let's compare the performance of all the clustering methods we've explored: K-means, Hierarchical Clustering (with different distance metrics), and GMM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f42208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply K-means clustering with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
    "kmeans_labels = kmeans.fit_predict(X_scaled)\n",
    "kmeans_ari = adjusted_rand_score(y_true, kmeans_labels)\n",
    "\n",
    "# Use the best hierarchical clustering result from Task 2\n",
    "best_metric_idx = np.argmax(ari_scores)\n",
    "best_metric = dist_metrics[best_metric_idx]\n",
    "best_hc_ari = ari_scores[best_metric_idx]\n",
    "\n",
    "# For DBSCAN, we need to find the appropriate epsilon value\n",
    "# Use nearest neighbors to estimate a good epsilon value\n",
    "neighbors = NearestNeighbors(n_neighbors=2)\n",
    "neighbors_fit = neighbors.fit(X_scaled)\n",
    "distances, indices = neighbors_fit.kneighbors(X_scaled)\n",
    "distances = np.sort(distances[:, 1])\n",
    "\n",
    "# Plot k-distance graph to find the \"elbow\"\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.title('K-Distance Graph for DBSCAN Epsilon Estimation', fontsize=16)\n",
    "plt.xlabel('Points sorted by distance', fontsize=14)\n",
    "plt.ylabel('Distance to 2nd nearest neighbor', fontsize=14)\n",
    "plt.axhline(y=0.5, color='r', linestyle='--')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Apply DBSCAN with the estimated epsilon\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)\n",
    "dbscan_labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "# Count number of clusters found (excluding noise points with label -1)\n",
    "n_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\n",
    "print(f\"DBSCAN found {n_clusters} clusters and {sum(dbscan_labels == -1)} noise points\")\n",
    "\n",
    "# Calculate ARI for DBSCAN (only if it found some clusters)\n",
    "if n_clusters > 0:\n",
    "    dbscan_ari = adjusted_rand_score(y_true, dbscan_labels)\n",
    "else:\n",
    "    dbscan_ari = 0\n",
    "\n",
    "# Create a comparison table of all methods\n",
    "comparison = pd.DataFrame({\n",
    "    'Clustering Method': ['K-means', f'Hierarchical ({best_metric})', 'GMM', 'DBSCAN'],\n",
    "    'Adjusted Rand Index': [kmeans_ari, best_hc_ari, ari_gmm_scores[1], dbscan_ari]\n",
    "}).sort_values(by='Adjusted Rand Index', ascending=False)\n",
    "\n",
    "print(\"\\nClustering Methods Comparison (higher ARI is better):\")\n",
    "display(comparison)\n",
    "\n",
    "# Visualize all clustering results side by side\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# Plot K-means clusters\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=kmeans_labels, \n",
    "            cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "plt.title(f'K-means (ARI: {kmeans_ari:.3f})', fontsize=14)\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot Hierarchical clusters (best metric)\n",
    "method = 'ward' if best_metric == 'euclidean' else 'complete'\n",
    "if best_metric == 'euclidean':\n",
    "    Z = linkage(X_scaled, method=method, metric=best_metric)\n",
    "else:\n",
    "    dist_matrix = pdist(X_scaled, metric=best_metric)\n",
    "    Z = linkage(dist_matrix, method=method)\n",
    "hc_labels = fcluster(Z, t=3, criterion='maxclust')\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=hc_labels, \n",
    "            cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "plt.title(f'Hierarchical ({best_metric}) (ARI: {best_hc_ari:.3f})', fontsize=14)\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot GMM clusters\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=gmm_labels, \n",
    "            cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "plt.title(f'GMM (ARI: {ari_gmm_scores[1]:.3f})', fontsize=14)\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot DBSCAN clusters\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=dbscan_labels, \n",
    "            cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "plt.title(f'DBSCAN (ARI: {dbscan_ari:.3f})', fontsize=14)\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot actual species for reference\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(pca_result[:, 0], pca_result[:, 1], c=y_true, \n",
    "            cmap='viridis', s=80, alpha=0.8, edgecolor='k')\n",
    "plt.title('Actual Penguin Species', fontsize=14)\n",
    "plt.xlabel('PC1', fontsize=12)\n",
    "plt.ylabel('PC2', fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e689af",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Dimensionality Reduction**: \n",
    "   - t-SNE generally provided better visual separation of penguin species compared to PCA, especially for distinguishing between Adelie and Chinstrap species.\n",
    "   - While PCA preserves global structure, t-SNE better preserves local structure, which is particularly useful for clustering visualization.\n",
    "\n",
    "2. **Distance Metrics in Hierarchical Clustering**:\n",
    "   - Different distance metrics produced notably different clustering results.\n",
    "   - The best-performing metric (based on the ARI scores from our analysis) most accurately recovered the natural penguin species groupings.\n",
    "   - This demonstrates the importance of choosing the right distance metric for the specific data structure.\n",
    "\n",
    "3. **Clustering Algorithms Comparison**:\n",
    "   - GMM provided probabilistic cluster assignments, adding more nuance than hard clustering approaches.\n",
    "   - GMM generally performed well, capturing the elliptical nature of the penguin clusters.\n",
    "   - The optimal number of clusters identified by BIC/AIC may differ from the known number of species.\n",
    "   - DBSCAN's performance depended heavily on parameter selection (eps and min_samples).\n",
    "\n",
    "### Method Strengths and Weaknesses\n",
    "\n",
    "- **K-means**: Simple and efficient, but assumes spherical clusters and requires knowing k in advance.\n",
    "- **Hierarchical Clustering**: Provides a dendrogram showing relationships between clusters, but sensitive to the chosen distance metric and linkage method.\n",
    "- **GMM**: Captures elliptical clusters and provides probability distributions, but may overfit with too many components.\n",
    "- **DBSCAN**: Can find arbitrarily shaped clusters and identify noise points, but parameter selection is challenging.\n",
    "\n",
    "### Biological Interpretation\n",
    "\n",
    "The clustering algorithms successfully identified patterns in penguin measurements that largely correspond to taxonomic species classifications. This suggests that morphological measurements captured in the dataset are strongly associated with evolutionary divergence between penguin species.\n",
    "\n",
    "The cases where clustering algorithms assigned penguins to different groups than their taxonomic classification could represent:\n",
    "1. Measurement errors or outliers\n",
    "2. Natural variation within species\n",
    "3. Possible hybridization between similar species\n",
    "\n",
    "These findings demonstrate how machine learning can complement traditional taxonomic approaches in biological classification."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
